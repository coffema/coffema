{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coffema/coffema/blob/main/Emoji_Sentiment_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU78WMPocdFw"
      },
      "source": [
        "# Assignment 03: Emoji Sentiment Classification\n",
        "\n",
        "This Week's assignment is to train sequence models on the Emoji Data to classify the sentences emotion. You'll be creating models that takes in a sentence and predicts the appropriate emoji that describes the sentiment.\n",
        "\n",
        "Before starting copy this file and work on your own copy by following the below steps:\n",
        "* `File > Save Copy in Drive`.\n",
        "* Add your name to the file (e.g., Assignment 01 - Zahraa Dhafer)\n",
        "\n",
        "Before submitting do the following:\n",
        "* Rerun the entire notebook from beginning to end to ensure everything is working properly and there are no errros `Runtime > Restart and run all`\n",
        "* Make sure the outputs of your code matches the expected outputs.\n",
        "* Download the notebook locally `File > Download > Download .ipynb`\n",
        "* Submit it through the submission form listed below.\n",
        "\n",
        "**Make sure to only edit cells that start with `# YOUR CODE HERE` and nothing else, if you need more space to write your code you can add more cells, but never delete any existing cells.**\n",
        "\n",
        "**Submission Deadline: Saturday, 18/2/2023 at 11 PM**\n",
        "\n",
        "**Submission Form:**  https://forms.gle/DpgK8sHDfUSLZFmbA\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "1. Read & Preprocess Dataset\n",
        "   1. Text Cleanup\n",
        "   2. Vectorize Text\n",
        "   3. Padding\n",
        "2. Build and train an RNN-based model\n",
        "3. Save and test the model\n",
        "\n",
        "Good luck and feel free to ask any questions on the #questions channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kJY4cXPY8a9"
      },
      "outputs": [],
      "source": [
        "# Import python libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import re  \n",
        "import string  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hw7ER2E3K2G"
      },
      "outputs": [],
      "source": [
        "# To ensure notebook's reproducability, let's set the random seed\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nd1HP4sZL99"
      },
      "source": [
        "## Data Preperation\n",
        "\n",
        "\n",
        "**DATASET**\n",
        "The dataset consists of two csv files, a training file with 16k rows and a testing file with 2k rows, each row has 3 columns, the sentence, the emotion as text (meant to provide description to the emoji and not to be used in training/testing) and the emoji symbol (e.g. üòÑ, üò°, üòç).<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL5LKk0cSFNn",
        "outputId": "7036b198-dadf-4fea-f192-6753b738a0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-18 21:08:12--  https://pub-7bf6c3e2f2bf41b083bd3e31313d5856.r2.dev/emoji_dataset.zip\n",
            "Resolving pub-7bf6c3e2f2bf41b083bd3e31313d5856.r2.dev (pub-7bf6c3e2f2bf41b083bd3e31313d5856.r2.dev)... 104.18.3.35, 104.18.2.35, 2606:4700::6812:323, ...\n",
            "Connecting to pub-7bf6c3e2f2bf41b083bd3e31313d5856.r2.dev (pub-7bf6c3e2f2bf41b083bd3e31313d5856.r2.dev)|104.18.3.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 659306 (644K) [application/zip]\n",
            "Saving to: ‚Äòemoji_dataset.zip.1‚Äô\n",
            "\n",
            "emoji_dataset.zip.1 100%[===================>] 643.85K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-02-18 21:08:13 (30.8 MB/s) - ‚Äòemoji_dataset.zip.1‚Äô saved [659306/659306]\n",
            "\n",
            "Archive:  emoji_dataset.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!wget https://pub-7bf6c3e2f2bf41b083bd3e31313d5856.r2.dev/emoji_dataset.zip\n",
        "!unzip 'emoji_dataset.zip'   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtSLjjWktvh4"
      },
      "source": [
        "## Read Dataset\n",
        "\n",
        "Read the csv files into Pandas DataFrames `train_df` and `test_df`, note that the data is already split into training and testing sets, so you don't need to split it yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC9IVvoazNll"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh9CzoZ6t8bS"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"test.csv\")\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRcM1sS_VyML"
      },
      "outputs": [],
      "source": [
        "# Test (Do not edit)\n",
        "\n",
        "print(\"train_df shape: \", train_df.shape)\n",
        "print(\"test_df shape: \", test_df.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABTKnnAsjpCf"
      },
      "source": [
        "Expected Output\n",
        "\n",
        "```\n",
        "train_df shape:  (16000, 3)\n",
        "test_df shape:  (2000, 3)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwm0VDF3tvh5"
      },
      "outputs": [],
      "source": [
        "# let's preview the first 5 rows of the training dataset\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiyHzXPGk5nS"
      },
      "source": [
        "Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeAAaVKvtvh5"
      },
      "source": [
        "Notice that there are 3 columns in the dataset:\n",
        "* `text`: the sentence\n",
        "* `emotion`: the emotion as text (meant to provide description to the emoji and not to be used in training/testing)\n",
        "* `emoji`: the emoji symbol (e.g. üòÑ, üò°, üòç)\n",
        "\n",
        "We will be using the `text` column as the input to our model and the `emoji` column as the target.\n",
        "\n",
        "Split the data into `x_train`, `y_train`, `x_test`, `y_test` using the `text` and `emoji` columns respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6To2jvvEzRmz"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "x_train = train_df['text']\n",
        "y_train = train_df['emoji']\n",
        "x_test = test_df['text']\n",
        "y_test = test_df['emoji']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7jmQxqYjfRe"
      },
      "outputs": [],
      "source": [
        "# Test (Do not edit)\n",
        "\n",
        "print(\"Number of train_text: \", len(x_train))\n",
        "print(\"Number of test_text:\", len(x_test))\n",
        "print(\"Labels: \", sorted(set(y_train)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQdi90hxtvh5"
      },
      "source": [
        "Expected Output\n",
        "\n",
        "```\n",
        "Number of train_text:  16000\n",
        "Number of test_text: 2000\n",
        "Labels:  ['üòÑ', 'üòç', 'üò°', 'üò¢', 'üò®', 'üò≤']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbkGa2lwrZ-A"
      },
      "source": [
        "## Preprocess the text\n",
        "\n",
        "Let's start by setting up hyperparameters preprocessing the text and the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7RKvlFZm4p1"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000 # The maximum number of words to be used. (most frequent)\n",
        "max_sequence = 24 # Max number of words in each text.\n",
        "batch_size = 64 # Batch size for training.\n",
        "embedding_dims = 50 # Dimension of the embedding layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tu_IFIztvh5"
      },
      "source": [
        "Create function `text_cleanup` that takes a string as input and returns a string after removing all the special characters and numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEF28BU3jCIP"
      },
      "outputs": [],
      "source": [
        "# Create a function to clean up the text \n",
        "\n",
        "def text_cleanup(text):\n",
        "\n",
        "    text = re.sub(r\"<.*?>\", \"\", text) # remove HTML tags\n",
        "    text = re.sub(r\"\\d+\", \"\", text) # remove numbers\n",
        "    text = re.sub(r\"\\w*\\d\\w*\", \"\", text) # remove words with numbers\n",
        "    text = re.sub(r\"https?://\\S+\", \"\", text) # remove URLs\n",
        "    text = re.sub(r\"\\S*@\\S*\\s?\", \"\", text) # remove emails\n",
        "    text = re.sub(r\"@\\S+\", \"\", text) # remove mentions (@username)\n",
        "    text = re.sub(r\"#\\S+\", \"\", text) # remove hashtags (#)\n",
        "    text = re.sub(r\"\\s+\", \" \", text) # remove extra spaces\n",
        "    \n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-6oMYSWtvh6"
      },
      "source": [
        "Apply the function to the `x_train` and `x_test` and save the results in `x_train_clean` and `x_test_clean` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbpybQaVtvh6"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "x_train_clean = x_train.map(text_cleanup)\n",
        "\n",
        "x_test_clean = x_test.map(text_cleanup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8BTFOQCtvh6"
      },
      "source": [
        "### Vectorize Text\n",
        "\n",
        "Create a `tf.keras.preprocessing.text.Tokenizer` named `tokenizer` and apply the approriate parameters to it (make sure to include the `oov_token` parameter). Then fit the tokenizer on the `x_train_clean` data.\n",
        "\n",
        "Don't convert the text to sequences yet, we will do that later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yqwh-sV9n_eB"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=vocab_size,\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    char_level=False,\n",
        "    oov_token=\"<OOV>\",\n",
        "    analyzer=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVFxkW4thu92"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(x_train_clean)\n",
        "tokenizer.fit_on_texts(x_test_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHBHZTMC32y2"
      },
      "outputs": [],
      "source": [
        "# Test (Do not edit)\n",
        "\n",
        "print(\"OOV token: \", tokenizer.oov_token)\n",
        "print(\"Vocabulary size: \", tokenizer.get_config()['num_words'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0gfMRjytvh6"
      },
      "source": [
        "Expected Output\n",
        "\n",
        "```\n",
        "OOV token:  <OOV>\n",
        "Vocabulary size:  15212\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85vi0feWtvh6"
      },
      "source": [
        "Now that we have a tokenizer, we can use it to convert the text to sequences. Create a function `text_to_sequences` that takes a list of strings as input and returns a list of lists of integers (the sequences). Save the tokenized `x_train_clean` and `x_test_clean` in `x_train_seq` and `x_test_seq` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXUjpPnrUsCA"
      },
      "outputs": [],
      "source": [
        "x_train_seq = tokenizer.texts_to_sequences(x_train_clean)\n",
        "x_test_seq = tokenizer.texts_to_sequences(x_test_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rub8IQfjg8ug"
      },
      "outputs": [],
      "source": [
        "x_train_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoYCbfbHtvh6"
      },
      "source": [
        "### Padding\n",
        "\n",
        "Create a function `pad_sequences` that takes a list of lists of integers as input and returns a list of lists of integers after padding the sequences to the same length (use the maximum length that we already defined with the hyperparameter, make sure to set the `padding` parameter to `post`).\n",
        "\n",
        "Save the padded `x_train_seq` and `x_test_seq` in `x_train_pad` and `x_test_pad` respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY0Mj4sznyIz"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "x_train_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "x_train_seq , maxlen=max_sequence, padding=\"post\"\n",
        ")\n",
        "\n",
        "x_test_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "x_test_seq , maxlen=max_sequence, padding=\"post\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnFZ96Wytvh6"
      },
      "outputs": [],
      "source": [
        "# Test (Do not edit)\n",
        "\n",
        "print(\"x_train shape: \", x_train_pad.shape)\n",
        "print(\"x_test shape: \", x_test_pad.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhlfMk6ttvh6"
      },
      "source": [
        "Expected Output\n",
        "\n",
        "```\n",
        "x_train shape:  (16000, 24)\n",
        "x_test shape:  (2000, 24)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx8MnBFPtvh6"
      },
      "source": [
        "### One Hot Encode Labels\n",
        "\n",
        "Create a function `one_hot_encode` that takes a list of strings as input and returns a list of lists of integers after one hot encoding the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSOyp6GGrjEa"
      },
      "outputs": [],
      "source": [
        "# create emoji index and reverse index\n",
        "\n",
        "emoji_index = {'üòÑ': 0, 'üòç': 1, 'üò°': 2, 'üò¢': 3, 'üò®': 4, 'üò≤': 5}\n",
        "\n",
        "reverse_emoji_index = {v: k for k, v in emoji_index.items()}\n",
        "\n",
        "emoji_index, reverse_emoji_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4p42YzFtvh6"
      },
      "source": [
        "Now create two functions:\n",
        "* `emoji_to_index`: takes an emoji string as input and returns the index of the emoji using `emoji_index` dictionary.\n",
        "* `index_to_emoji`: takes an index as input and returns the emoji string using `index_emoji` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVVE60c8tvh7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "def emoji_to_index(emoji):\n",
        "  return emoji_index.get(emoji)\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVWHfurTjGTs"
      },
      "outputs": [],
      "source": [
        "def index_to_emoji(index):\n",
        "  return list(emoji_index.keys())[list(emoji_index.values()).index(index)]\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfwJUrxttvh7"
      },
      "outputs": [],
      "source": [
        "# Test (Do not edit)\n",
        "\n",
        "print(\"üòÑindex: \", emoji_to_index('üòÑ'))\n",
        "print(\"4 emoji: \", index_to_emoji(4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2mtUfGetvh7"
      },
      "source": [
        "Expected Output\n",
        "\n",
        "```\n",
        "üòÑ index:  0\n",
        "4 emoji:  üò®\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU0ouoVntvh7"
      },
      "source": [
        "Now we will encode the labels using the function we just created and then one hot encode them.\n",
        "\n",
        "Remember that we need to convert the labels to integers first before one hot encoding them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QmhJhwsZ2Ak7",
        "outputId": "221c085b-832b-4c57-f735-3d5882d9ca76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        None\n",
              "1        None\n",
              "2        None\n",
              "3        None\n",
              "4        None\n",
              "         ... \n",
              "15995    None\n",
              "15996    None\n",
              "15997    None\n",
              "15998    None\n",
              "15999    None\n",
              "Name: emoji, Length: 16000, dtype: object"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train = y_train.apply(emoji_to_index)\n",
        "y_test = y_test.apply(emoji_to_index)\n",
        "\n",
        "depth = len(set(y_train))\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC6VUf4Gtvh7"
      },
      "outputs": [],
      "source": [
        "# convert emoji to index\n",
        "y_train = y_train.apply(emoji_to_index)\n",
        "y_test = y_test.apply(emoji_to_index)\n",
        "\n",
        "# one-hot encode the labels\n",
        "depth = len(set(y_train))\n",
        "y_train = tf.one_hot(y_train, depth)\n",
        "y_test = tf.one_hot(y_test, depth)\n",
        "\n",
        "y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YL-23vetvh7"
      },
      "source": [
        "Now that we have the tensors ready, let's convert them to TF.Data pipelines. \n",
        "\n",
        "Remember that we can always use TF.Data Pipeplines even if we do the preprocessing manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RVexIANtvh7"
      },
      "outputs": [],
      "source": [
        "def dataset_creator(x, y):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_dataset = dataset_creator(x_train_pad, y_train)\n",
        "test_dataset = dataset_creator(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy1oJJb3tvh7"
      },
      "outputs": [],
      "source": [
        "# preview dataset\n",
        "for x, y in train_dataset.take(1):\n",
        "    print(x.shape, y.shape)\n",
        "    print(x[0])\n",
        "    print(y[0])\n",
        "\n",
        "# preview dataset size\n",
        "print(\"Train dataset size: \", len(train_dataset))\n",
        "print(\"Test dataset size: \", len(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOEo3I9grsab"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipR10c3WZqby"
      },
      "source": [
        "Create `model` using the sequential API. Your target is hit 90% accuracy on the validation set.\n",
        "\n",
        "Start with the smallest model that you can think of and then increase model size until you hit the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bzlm4i-Zqb7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.LSTM(\n",
        "            32, input_shape=(sequence_length, len(selected_columns))\n",
        "        ),  # input shape is sequence_length (720) x number of features (7)\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.Huber(),\n",
        "    metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
        ")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"model_checkpoint.h5\",\n",
        "    save_best_only=True,\n",
        "    monitor=\"val_loss\",\n",
        ")\n",
        "\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMJJp5FhZqb7"
      },
      "outputs": [],
      "source": [
        "# Test (Do not edit)\n",
        "\n",
        "model.summary()    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXUcwbGqtvh7"
      },
      "source": [
        "Expected Output\n",
        "\n",
        "```\n",
        "Model: \"...\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " embedding (Embedding)       (None, 24, 50)            500050    \n",
        "                                                                 \n",
        " ...\n",
        "                                                                 \n",
        " dense (Dense)               (None, 6)                 ...       \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: ...\n",
        "Trainable params: ...\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnoVml9RDM6d"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(optimizer= tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd2-xsw4tvh7"
      },
      "source": [
        "Now fit the model on the training data and evaluate it on the testing data. Use an approriate number of epochs and any callbacks you deem necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9EmaDe-pX6B"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStsV_TKtvh7"
      },
      "source": [
        "You will now save the model to `model.h5` and then load it again to make sure it works.\n",
        "\n",
        "Make sure to save the whole model and not just weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZd2LLEgtvh7"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arz3beIBtvh7"
      },
      "outputs": [],
      "source": [
        "# Test (Do not edit)\n",
        "\n",
        "# delete existing model and load it from scratch\n",
        "if not model:\n",
        "    del model\n",
        "model = tf.keras.models.load_model('model.h5')\n",
        "\n",
        "# input a test sentence and get the predicted emoji\n",
        "def predict_emoji(sentence):\n",
        "    sentence = text_cleanup(sentence)\n",
        "    sentence = tokenizer.texts_to_sequences([sentence])\n",
        "    sentence = tf.keras.preprocessing.sequence.pad_sequences(sentence, maxlen=max_sequence, padding='post')\n",
        "    prediction = model.predict(sentence, verbose=0)\n",
        "    return index_to_emoji(np.argmax(prediction))\n",
        "\n",
        "sentence1 = \"I am so happy\"\n",
        "sentence2 = \"I am so sad\"\n",
        "\n",
        "print(\"Sentence 1: \", sentence1, \"\\nEmoji: \", predict_emoji(sentence1))\n",
        "print()\n",
        "print(\"Sentence 2: \", sentence2, \"\\nEmoji: \", predict_emoji(sentence2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJrtcUAOtvh7"
      },
      "source": [
        "Expected Output\n",
        "\n",
        "```\n",
        "Sentence 1:  I am so happy \n",
        "Emoji:  üòÑ\n",
        "\n",
        "Sentence 2:  I am so sad \n",
        "Emoji:  üò¢\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "effefe004be52d4cd3a12856ff0d4a1b800b83fc4bd48cce66e2ad043e78af0c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}